{"/docs/architecture/system_overview.md":"# System Architecture\n\nThe architecture of NeuroxAI mimics the macro-scale connectivity of the mammalian brain. The simulation runs in discrete timesteps (default `dt = 0.1ms`) to capture the fine temporal resolution of action potentials.\n\n## Core Loop (`update` cycle)\n\nThe central update loop ensures proper causality and data flow between subsystems.\n\n1.  **Sensory Processing (Bottom-Up)**\n    *   **V1 (Visual Cortex)**: Processes raw visual input via Gabor filters (orientation selectivity). Accelerated by CUDA.\n    *   **Cochlea**: Converts audio streams into spike trains via frequency decomposition.\n    *   **Somatosensory**: Barrel cortex processes tactile input from the spatial system.\n\n2.  **Thalamic Integration**\n    *   The **Thalamus** receives these sensory inputs and acts as a gateway.\n    *   It is modulated by **Attention** (Top-Down) and **Cortical Feedback**. Only salient information is relayed to the cortex.\n\n3.  **Cortical Processing (Predictive Hierarchy)**\n    *   Information flows up the hierarchy: `V1 -> V2 -> V4 -> IT -> PFC`.\n    *   Simultaneously, predictions flow down. The difference (Prediction Error) drives learning and attention.\n    *   **Working Memory** maintains active patterns via recurrent excitation, gated by Acetylcholine.\n\n4.  **Subcortical Loops**\n    *   **Hippocampus**: Receives high-level cortical patterns. Rapidly encodes \"snapshots\" of the state (Episodic Memory).\n    *   **Amygdala**: Monitors sensory streams for threat/salience. Modulates global arousal (Norepinephrine) if threats are detected.\n    *   **Basal Ganglia**: Receives state context. Selects the most valuable Action via Dopamine-modulated Go/NoGo pathways.\n    *   **Cerebellum**: Computes motor errors and fine-tunes motor commands before execution.\n\n5.  **Learning & Plasticity**\n    *   **STDP**: Synaptic weights are updated based on the relative timing of pre- and post-synaptic spikes.\n    *   **Homeostasis**: Neurons adjust their excitability to maintain a target firing rate (~5Hz).\n    *   **Sleep**: When the system enters \"sleep mode\", the Hippocampus replays memories. These are consolidated into the cortex, and synaptic scaling normalizes weights.\n\n## Data Flow Diagram\n\n```mermaid\n    graph TD\n    Input[Sensory Input] --> Thalamus\n    Thalamus --> Cortex\n    Cortex <-->|Loops| Hippocampus[Hippocampus]\n    Cortex --> BG[Basal Ganglia]\n    Cortex --> Amygdala\n    \n    BG -->|Action Selection| Thalamus\n    BG -->|Dopamine| Neuromodulation\n    \n    Amygdala -->|Arousal| Neuromodulation\n    \n    Cortex --> Cerebellum\n    Cerebellum -->|Error Correction| Cortex\n    \n    Neuromodulation -.->|Global Broadcast| All[All Systems]\n```\n\n## GPU Acceleration Strategy\n\nTo achieve real-time performance with millions of neurons, NeuroxAI employs a hybrid CPU-GPU strategy:\n\n*   **CPU (Rust/Rayon)**: Handles complex logic, sparse connectivity updates, and sequential decision making (Basal Ganglia, High-level Cognition).\n*   **GPU (CUDA)**: Handles massive parallel operations:\n    *   **Dense Convolution**: V1 visual processing (Gabor filters).\n    *   **Motion Processing**: Optical flow computation (MT/MST).\n    *   **Cognitive Kernels**: Large-scale attention matrix multiplication.\n\nData transfer between Host and Device is minimized by batching inputs and keeping sensory state resident on the GPU where possible.\n","/docs/cli_reference.md":"# CLI Reference\n\nNeuroxAI provides a powerful Command Line Interface for interaction, training, and benchmarking.\n\n## Chat Interface (`chat`)\n\nThe primary mode for interacting with the cognitive architecture.\n\n```bash\nneurox-ai chat [OPTIONS]\n```\n\n### Configuration Arguments\n*   `--vocab <N>`: Sets the size of the active vocabulary (default: `10000`).\n*   `--pattern-dim <N>`: Dimensionality of the hyperdimensional vectors used for semantic representation (default: `512`).\n*   `--neurons <N>`: Total number of neurons instantiated in the brain simulation (default: `10000`).\n*   `--context <N>`: Size of the sliding context window (working memory capacity) (default: `128`).\n*   `--sensitivity <FLOAT>`: Scaling factor for dopamine release, affecting how strongly the system learns from rewards (default: `1.0`).\n\n## Problem Solving (`solve`)\n\nDirect access to symbolic solving modules for mathematics and chemistry.\n\n```bash\nneurox-ai solve --problem-type <TYPE> \"<PROBLEM>\"\n```\n\n### Examples\n\n#### Mathematics\nSolve algebraic equations or arithmetic expressions:\n```bash\nneurox-ai solve --problem-type math \"2 * x + 5 = 15\"\n# Output: x = 5\n```\n\n#### Chemistry\nBalance chemical equations using the stoichiometric solver:\n```bash\nneurox-ai solve --problem-type chemistry \"H2 + O2 -> H2O\"\n# Output: 2H2 + O2 -> 2H2O\n\nneurox-ai solve --problem-type chemistry \"C6H12O6 + O2 -> CO2 + H2O\"\n# Output: C6H12O6 + 6O2 -> 6CO2 + 6H2O\n```\n\n*   **Note**: Use quotes around the problem string to ensure special characters (like `*` or `->`) are handled correctly by your shell.\n\n\n\n## Benchmarking (`benchmark`)\n\nTools for verifying system performance and accuracy.\n\n```bash\nneurox-ai benchmark [OPTIONS]\n```\n\n*   `--data-dir`: Path to MNIST data or \"synthetic\" / \"auto\".\n*   `--epochs`: Number of training iterations.\n*   `--bits`: Quantization level (4 or 8).\n*   `--neurons`: Hidden layer size.\n","/docs/internals/data_training.md":"# Data & Training Pipeline\n\nThe \"personality\" and knowledge of NeuroxAI are encoded in its training data. Unlike traditional LLMs trained on raw internet text, NeuroxAI uses a structured \"Semantic Genome\".\n\n## The Semantic Genome (`data/czech_training.json`)\n\nThis massive JSON file (>11,000 lines) defines the ontological structure of the agent's world.\n\n### 1. Neuro-Impact Definitions\nWords are not just tokens; they are chemical triggers.\n```json\n\"láska\": {\n    \"valence\": 0.9,\n    \"neuro_impact\": {\n        \"oxytocin\": 1.0,    // Bonding\n        \"dopamine\": 0.5     // Reward\n    }\n},\n\"strach\": {\n    \"valence\": -0.8,\n    \"neuro_impact\": {\n        \"norepinephrine\": 0.8 // Stress/Arousal\n    }\n}\n```\nWhen the agent processes these words, its internal neuromodulator levels shift, altering learning rates and decision making.\n\n### 2. Pragmatic Rules\nDefines conversational logic and flow.\n*   `Greeting -> Greeting`\n*   `Question -> Explanation`\n*   `Insult -> Emotional Response`\n\n### 3. Sentence Templates\nGrammatical structures for the **IFG Syntactic Planner**.\n*   `[Pronoun, Verb, Noun]` -> \"Já mám hlad\"\n*   `[Interjection, Pronoun, Verb]` -> \"Ahoj, já jsem...\"\n\n## Training Process (`src/training/`)\n\nThe training pipeline transforms this static data into dynamic synaptic weights.\n\n### 1. Hebbian Association\nWords that appear together in templates develop strong synaptic connections (Skip-gram equivalent in SNN).\n\n### 2. Triplet STDP\nTemporal sequences are learned via Spike-Timing-Dependent Plasticity.\n*   **Pre-Post spike**: Strengthens connection (Causal).\n*   **Post-Pre spike**: Weakens connection (Acausal).\n\n### 3. Consolidation (Sleep)\nEvery $N$ epochs, the system enters a \"Sleep\" state.\n*   **Replay**: High-error samples are re-processed with boosted plasticity.\n*   **Synaptic Scaling**: All weights are multiplicatively scaled down ($w \\cdot 0.98$) to prevent runaway growth.\n*   **Pruning**: Synapses below a threshold ($0.1$) are removed to maintain sparsity.\n\n### 4. Export & Quantization\nThe final model can be exported with **8-bit Quantization Awareness**.\n*   Weights are clamped and discretized to `int8` range.\n*   This allows efficient deployment on edge devices.\n","/docs/internals/gpu_acceleration.md":"# GPU Acceleration (CUDA)\n\nNeuroxAI leverages NVIDIA GPUs to achieve 100x+ speedups over CPU execution for large-scale simulations (1M+ neurons). The implementation uses raw CUDA kernels compiled at runtime via `cudarc`.\n\n## Architecture\n\nThe system uses a hybrid approach:\n\n* **CPU**: Complex logic, branching, sparse graph management.\n* **GPU**: Massive parallel neuron updates, dense sensory processing.\n\n## Kernels (`src/cuda/kernels.rs`)\n\n### 1. LIF Update Kernel (`lif_update`)\n\nImplements the differential equation for Leaky Integrate-and-Fire neurons in parallel.\n\n```c\n// dV/dt = (-V + R*I) / tau_m\nfloat dv = ((-v + r_m * i_input) / tau_m) * dt;\nv += dv;\nv = fminf(fmaxf(v, -100.0f), 50.0f); // Clamping\n```\n\n* **Threads**: One thread per neuron.\n* **Optimizations**: Coalesced memory access for state arrays (`v`, `threshold`, `tau`).\n\n### 2. Temporal Fusion LIF (`temporal_fusion_lif_update`)\n\n**Crucial Optimization**: Standard SNN simulation is memory-bandwidth bound. We use **Temporal Fusion** (based on arXiv Aug 2024) to compute multiple timesteps in a single kernel launch.\n\n* **Mechanism**: Loads neuron parameters *once* into registers, then loops for $T$ timesteps.\n* **Benefit**: Reduces global memory reads by ~60%.\n* **Speedup**: ~2.8x on RTX 3070 compared to naive kernel.\n\n### 3. Sparse Spike Propagation (`spike_propagation`)\n\nHandles the connectivity graph using **CSR (Compressed Sparse Row)** format.\n\n* **Logic**:\n    1. Post-synaptic thread reads `row_ptr` to find incoming connections.\n    2. Iterates `col_idx` to find pre-synaptic neurons.\n    3. Checks `spike_flags` array (dense bitmask).\n    4. Accumulates weights.\n\n### 4. Triplet STDP Kernel (`triplet_stdp_update`)\n\nUpdates synaptic weights in parallel. This is computationally expensive on CPU due to $O(N_{synapses})$ complexity.\n\n* **Strategy**: Event-driven update. Threads only process synapses where the *post-synaptic* neuron has spiked.\n* **Rule**: `Δw = -lr_pre * a_post1 + lr_post * a_pre * a_post2`\n\n### 5. Spiking Convolutional Kernels (`src/cuda/spiking_conv_kernels.rs`)\n\nImplemented in **NeuroxAI 2.0** (2025 upgrade). These kernels enable full Spiking CNNs on GPU.\n\n* **`GpuSpikingConv2D`**: Fused Convolution + LIF dynamics. Keeps all state (membrane potential) in VRAM.\n* **`GpuSpikeMaxPool`**: Spatiotemporal pooling that propagates the strongest spikes.\n\nSee [Spiking CNN Module](../modules/spiking_cnn.md) for full details.\n\n## V1 Visual Processing (`src/cuda/v1_kernels.rs`)\n\nWe implement a dense Gabor filter bank on GPU for visual inputs.\n\n* **Input**: 128x128 retinal image.\n* **Filters**: 4 Orientations (0°, 45°, 90°, 135°).\n* **Performance**: < 2ms latency (vs ~200ms on CPU).\n\n## Memory Management\n\n* **Zero-Copy**: Where possible, we map host memory to device (pinned memory).\n* **Batching**: Inputs are batched to saturate the GPU (min batch size ~10k neurons).\n* **Precision**: All computations use `f32` (single precision) for balance between speed and biological accuracy.\n","/docs/internals/language_cognition.md":"# Cognitive Architecture & Language\n\nNeuroxAI moves beyond simple \"input-output\" mapping by simulating the anatomical structures responsible for language and higher-order thought.\n\n## Dual-Stream Language Model\n\nBased on the Hickok & Poeppel (2007) cortical model of speech processing.\n\n### 1. Ventral Stream (The \"What\" Pathway)\nResponsible for mapping sound to meaning.\n*   **STG (Superior Temporal Gyrus)**: Initial phonological analysis.\n*   **MTG (Middle Temporal Gyrus)**: Lexical interface.\n*   **ATL (Anterior Temporal Lobe)**: The **Semantic Hub**. Contains \"Concept Cells\" that bind multimodal features (visual, auditory, emotional) into a unified concept.\n*   **Embeddings**: We do not use static vectors (like Word2Vec). Embeddings are learned dynamically via Hebbian association between token firing patterns.\n\n### 2. Dorsal Stream (The \"How\" Pathway)\nResponsible for mapping sound to articulation.\n*   **Spt (Sylvian-parietal-temporal)**: Sensorimotor translation.\n*   **IFG (Inferior Frontal Gyrus / Broca's Area)**: Articulatory planning and syntactic sequencing.\n*   **Phonological Loop**: Implemented as a recurrent Working Memory buffer.\n\n## Metacognition (System 2)\n\nA supervisor system that monitors the primary cognitive loops.\n\n### Confidence & Uncertainty\nThe system continuously estimates its own certainty.\n*   $Uncertainty = 1.0 - Confidence$\n*   If $Uncertainty > Threshold$, the system inhibits immediate response and engages **Information Seeking** behavior.\n\n### Cognitive Strategies\nThe agent dynamically selects a strategy based on:\n1.  **Complexity**: Estimated from prediction error.\n2.  **Time Constraint**: How much time is available?\n\n| Strategy | Cost | Time | Use Case |\n|----------|------|------|----------|\n| **FastIntuitive** | Low | Fast | Routine conversation |\n| **MemoryRetrieval** | Med | Med | Factual queries |\n| **ChainOfThought** | High | Slow | Logical puzzles |\n| **ProblemDecomposition** | V. High | V. Slow | Novel scenarios |\n\n### Self-Correction\nIf an error is detected (e.g., mismatch between predicted and actual outcome), the Metacognition module:\n1.  Reduces confidence in the current strategy.\n2.  Updates the `StrategyRecord` (Reinforcement Learning on strategies).\n3.  Triggers a strategy switch (e.g., from Intuitive to Analytical).\n","/docs/internals/neuromodulation.md":"# Neuromodulatory Systems\n\nThe brain is not a static graph; it is a dynamical system tuned by chemical broadcasters. NeuroxAI simulates four major neuromodulators that fundamentally alter computation and learning.\n\n## 1. Dopamine (DA): Reward & Precision\n\nDopamine is the primary driver of reinforcement learning (R-STDP) and action selection (Basal Ganglia).\n\n*   **Role**: Signals **Reward Prediction Error (RPE)**.\n*   **Mechanism**:\n    *   Modulates the amplitude of STDP weight changes.\n    *   In the Basal Ganglia, high DA promotes \"Go\" (D1 receptors), low DA promotes \"NoGo\" (D2 receptors).\n*   **Opponent Processing**: Computed relative to Serotonin: $Value = DA \\cdot Reward - 5HT \\cdot Punishment$.\n\n## 2. Serotonin (5-HT): Patience & Punishment\n\nRegulates the time horizon of decision making.\n\n*   **Role**: Temporal Discounting ($\\gamma$).\n*   **Dynamics**:\n    *   **High 5-HT**: Patient state ($\\gamma \\to 0.99$). The agent values future rewards almost as much as immediate ones.\n    *   **Low 5-HT**: Impulsive state ($\\gamma \\to 0.90$). The agent seeks immediate gratification.\n*   **Updates**: Increases slowly with successful long-term outcomes (`tau = 2000ms`).\n\n## 3. Acetylcholine (ACh): Attention & Encoding\n\nSwitches the cortex between \"Learning\" and \"Retrieval\" modes.\n\n*   **Role**: Encoding vs. Consolidation switch.\n*   **Mechanism**:\n    *   **High ACh (Encoding)**: Boosts afferent sensory input, suppresses feedback. Increases effective learning rate $\\eta$.\n    *   **Low ACh (Consolidation)**: Occurs during \"Sleep\" or low attention. Facilitates hippocampal replay and transfer to cortex.\n*   **Dynamics**: Triggered by the Attention System ($\\tau = 1000ms$).\n\n## 4. Norepinephrine (NE): Arousal & Exploration\n\nRegulates the Exploration-Exploitation trade-off via the **LC-NE** (Locus Coeruleus) system.\n\n*   **Role**: Global Gain & Randomness.\n*   **Signal**: Driven by **Unexpected Uncertainty** (when prediction errors are surprisingly large).\n*   **Mechanism**:\n    *   Increases the \"temperature\" of the softmax action selection.\n    *   Adds an exploration bonus: $\\epsilon = 0.1 \\times NE$.\n*   **Dynamics**: Fast acting ($\\tau = 500ms$).\n\n## 5. Oxytocin (OXT): Trust & Stress Buffering\n\nA specialized modulator introduced in the 2025 update for social agents.\n\n*   **Role**: Social Bonding.\n*   **Mechanism**:\n    *   Dampens the NE stress response: $Stress_{effective} = Stress_{input} \\times (1 - 0.5 \\cdot OXT)$.\n    *   Gating factor for \"intimate\" or high-trust language generation (lexicon filtering).\n","/docs/internals/plasticity.md":"# Synaptic Plasticity & Learning Rules\n\nNeuroxAI implements a biologically realistic suite of plasticity mechanisms, moving beyond simple backpropagation to simulate how biological synapses actually learn.\n\n## 1. Triplet STDP (Spike-Timing-Dependent Plasticity)\n\nUnlike standard Pair-based STDP (which fails to capture complex spike patterns), we implement the **Triplet STDP rule** (Pfister & Gerstner, 2006), which accounts for the frequency dependence of potentiation.\n\n### Mathematical Model\n\nThe weight change $\\Delta w$ depends on traces $r_1, r_2$ (presynaptic) and $o_1, o_2$ (postsynaptic).\n\n$$ \\frac{dw}{dt} = -A_{LTD} o_1 r_{det} + A_{LTP} r_1 o_2 $$\n\nWhere traces decay exponentially:\n$$ \\tau \\frac{dx}{dt} = -x + \\delta(t-t_{spike}) $$\n\n### Implementation Details (`src/learning/stdp.rs`)\n- **Presynaptic Trace ($\\tau_{pre}$)**: 20ms\n- **Postsynaptic Fast Trace ($\\tau_{post1}$)**: 20ms\n- **Postsynaptic Slow Trace ($\\tau_{post2}$)**: 40ms (Critical for triplet interactions)\n\nThis allows the network to learn temporal sequences and high-frequency bursts, achieving **93.8% accuracy on MNIST** with 4-bit weights (Nature SR 2025 reproduction).\n\n## 2. Reward-Modulated STDP (R-STDP)\n\nSolves the **Distal Reward Problem** (temporal credit assignment) by bridging the gap between synaptic activity and delayed dopamine release.\n\n### Mechanism: Eligibility Traces\nInstead of changing weights immediately, STDP events generate an **Eligibility Trace ($e_{ij}$)**.\n\n$$ \\tau_e \\frac{de_{ij}}{dt} = -e_{ij} + STDP(t) $$\n\nWeight changes occur only when dopamine ($DA$) is present:\n\n$$ \\frac{dw_{ij}}{dt} = \\eta \\cdot DA(t) \\cdot e_{ij}(t) $$\n\n### Implementation (`src/learning/rstdp.rs`)\n- **Trace Decay ($\\tau_e$)**: 1000ms (1 second window for reward association)\n- **Meta-Learning**: The learning rate $\\eta$ is dynamic.\n  - $\\eta = f(\\sigma_R)$ where $\\sigma_R$ is the variance of recent rewards.\n  - **High Uncertainty $\\to$ High Plasticity**.\n\n## 3. BCM Metaplasticity (Bienenstock-Cooper-Munro)\n\nPrevents runaway potentiation by introducing a sliding threshold for LTP/LTD induction.\n\n### Theory\nThe sign of synaptic change depends on postsynaptic activity $y$ relative to a threshold $\\theta_M$:\n\n$$ \\Delta w = \\eta \\cdot x \\cdot y \\cdot (y - \\theta_M) $$\n\n- If $y > \\theta_M \\to$ **LTP**\n- If $y < \\theta_M \\to$ **LTD**\n\nThe threshold itself adapts to the historical activity (homeostasis):\n$$ \\theta_M = \\langle y^2 \\rangle $$\n\n### Implementation (`src/learning/metaplasticity.rs`)\n- **Time Constant**: 10s - 1h adaptation window.\n- **Result**: Ensures selectivity stability. Neurons that fire too much become harder to potentiate.\n\n## 4. Calcium-Based Plasticity (Unified Rule)\n\nWe also provide a biophysical model based on postsynaptic Calcium ($Ca^{2+}$) concentration, unifying LTP and LTD into a single variable.\n\n$$ \\frac{d[Ca^{2+}]}{dt} = -\\frac{[Ca^{2+}]}{\\tau_{Ca}} + \\eta_{NMDA} I_{NMDA} + \\eta_{VDCC} I_{VDCC} $$\n\n- **LTP**: Triggered when $[Ca^{2+}] > \\theta_{high}$\n- **LTD**: Triggered when $\\theta_{low} < [Ca^{2+}] < \\theta_{high}$\n\nThis model (based on Chindemi et al., Nature Comm 2022) is used in our **Dendritic Neuron** implementation for accurate synaptic clustering.\n","/docs/modules/cognitive_upgrades.md":"# Cognitive Upgrades (2025)\n\nNeuroxAI incorporates several \"Human-Limit\" cognitive modules designed to emulate higher-order reasoning, self-awareness, and social intelligence. These go beyond standard SNN capabilities.\n\n## 1. Theory of Mind (`TheoryOfMind`)\nThis module allows the agent to simulate other agents' mental states.\n*   **Mechanism**: It maintains shadow copies of the self-model, parameterized by observed behaviors of others.\n*   **Function**: Enables prediction of social dynamics and cooperative planning.\n\n## 2. Inner Dialogue (`InnerDialogue`)\nSimulates the \"stream of consciousness\" or internal monologue.\n*   **Multi-Perspective Deliberation**: The system generates multiple competing \"thoughts\" or potential responses.\n*   **Arbitration**: A selection mechanism chooses the most coherent thought based on current goals and emotional state.\n\n## 3. Metacognition (System 2)\nImplements a supervisory attentional system that monitors the brain's own performance.\n*   **Uncertainty Estimation**: If the primary cortical prediction error is high, Metacognition engages.\n*   **Strategy Selection**: It can dynamically switch between \"Fast\" (heuristic, basal ganglia) and \"Slow\" (deliberative, cortical simulation) thinking modes.\n\n## 4. Emotional State Machine\nA continuous dynamical system representing the agent's affect.\n*   **Core Dimensions**: Valence (Good/Bad) and Arousal (Calm/Excited).\n*   **Impact**: Emotions are not just labels; they fundamentally alter simulation parameters.\n    *   *High Arousal* -> Increases Norepinephrine (Focus, Exploration).\n    *   *Negative Valence* -> Increases Serotonin (Patience, Withdrawal).\n","/docs/modules/cortical_hierarchy.md":"# Cortical Hierarchy & Predictive Coding\n\nNeuroxAI implements a 5-level cortical hierarchy based on the principles of Active Inference and Predictive Coding. This architecture allows the system to build generative models of the world.\n\n## Laminar Microcircuit\n\nEach \"cortical column\" in NeuroxAI is not just a single neuron, but a structured microcircuit representing cortical layers.\n\n| Layer | Function | Connectivity |\n|-------|----------|--------------|\n| **L4** | Input Receiver | Receives Bottom-Up input (Sensory or Lower Area). |\n| **L2/3** | Error Units | Computes Prediction Error: $E = Input - Prediction$. Sends Error Up. |\n| **L5** | Prediction Units | Generates Predictions. Sends Prediction Down. |\n| **L6** | Precision | Modulates the gain of Error Units (Attention). |\n\n## The Algorithm\n\nThe network minimizes **Free Energy** (Surprise) via dynamic updating of states.\n\n1.  **Top-Down Prediction**:\n    $$ \\mu_{level} = g(W \\cdot \\mu_{level+1}) $$\n2.  **Bottom-Up Error**:\n    $$ \\epsilon_{level} = \\Pi \\cdot (\\mu_{level-1} - \\mu_{level}) $$\n    Where $\\Pi$ is the **Precision** (inverse variance).\n3.  **State Update**:\n    Neurons update their firing rates to minimize $\\epsilon$.\n\n## Hierarchical Levels\n\n1.  **V1 (Visual Cortex)**:\n    *   **Features**: Edges, Orientations (Gabor filters).\n    *   **RF Size**: Small (5x5 px).\n2.  **V2**:\n    *   **Features**: Textures, Corners, Curvature.\n    *   **RF Size**: Medium.\n3.  **V4**:\n    *   **Features**: Simple shapes, Object parts.\n4.  **IT (Inferior Temporal)**:\n    *   **Features**: Whole objects, Identities.\n    *   **RF Size**: Large (covers most of visual field).\n5.  **PFC (Prefrontal Cortex)**:\n    *   **Features**: Categories, Rules, Context, Goals.\n    *   **Timescale**: Long (sustained activity).\n\n## Precision & Attention\n\nAttention in this framework is synonymous with **Precision Weighting**.\n*   **High Attention**: Increases gain on Error Units ($\\Pi \\uparrow$). The model trusts the sensory data more than its prediction.\n*   **Low Attention**: Decreases gain. The model ignores mismatch (e.g., during noise or imagination).\n","/docs/modules/mnist_benchmark.md":"# MNIST Benchmark Module\n\nThe `MNISTBenchmark` module allows for rigorous testing of the neuromorphic architecture's classification capabilities using the standard MNIST handwritten digit dataset. It supports both real data loading and synthetic data generation for quick prototyping.\n\n## CLI Usage\n\nThe benchmark is accessed via the `benchmark` subcommand:\n\n```bash\nneurox-ai benchmark [OPTIONS]\n```\n\n### Arguments\n\n*   `--data-dir <PATH>`: Directory containing MNIST files. Use `\"auto\"` to download automatically or `\"synthetic\"` (default) to generate procedural test data.\n*   `--epochs <N>`: Number of training passes (default: `10`).\n*   `--bits <4|8>`: Quantization precision for weight compression (default: `4`).\n*   `--neurons <N>`: Number of hidden layer neurons (default: `400`).\n*   `--duration <MS>`: Stimulus presentation time in milliseconds (default: `50.0`).\n*   `--isi <MS>`: Inter-stimulus interval (rest period) in milliseconds (default: `20.0`).\n\n## Synthetic Data Generation\n\nWhen `data_dir` is set to `\"synthetic\"`, the system procedurally generates digit-like patterns (circles for '0', lines for '1', etc.) with noise. This allows for:\n1.  Testing pipeline integrity without external downloads.\n2.  Verifying learning dynamics on controlled patterns.\n\n## Performance Metrics\n\nThe benchmark reports:\n1.  **FP32 Accuracy**: Baseline accuracy with full floating-point precision.\n2.  **Quantized Accuracy**: Accuracy after compressing weights to 4 or 8 bits.\n3.  **Compression Ratio**: Memory savings factor (e.g., 8.0x for 4-bit).\n4.  **Training Time**: Elapsed time per epoch.\n\n## Architecture\n\n*   **Input**: 784 Poisson spike trains (28x28 pixels).\n*   **Hidden**: Configurable (default 400) excitatory neurons with STDP.\n*   **Output**: 10 neurons (one per digit) with lateral inhibition (WTA).\n*   **Learning**: Reward-modulated STDP with homeostatic regulation.\n","/docs/modules/neuron_models.md":"# Neuron Models\n\nNeuroxAI does not use simple ReLU units. We simulate biologically grounded spiking neurons with complex internal dynamics.\n\n## LIF (Leaky Integrate-and-Fire)\n\nThe standard workhorse of SNNs.\n\n$$ \\tau_m \\frac{dV}{dt} = -(V - E_{leak}) + R_m \\cdot I_{input} $$\n\n*   **Spike**: When $V > V_{threshold}$, emit spike, reset $V \\to V_{reset}$.\n*   **Refractory Period**: Neuron cannot fire for 2-5ms after a spike.\n*   **Adaptation**: Threshold increases after spiking (Homeostasis), preventing seizure-like activity.\n\n## Izhikevich Neurons\n\nUsed for specific firing patterns required by different brain regions.\n$$ \\frac{dv}{dt} = 0.04v^2 + 5v + 140 - u + I $$\n$$ \\frac{du}{dt} = a(bv - u) $$\n\nSupported Types:\n*   **RS (Regular Spiking)**: Excitatory cortical neurons.\n*   **FS (Fast Spiking)**: Inhibitory interneurons (PV+).\n*   **IB (Intrinsically Bursting)**: Deep layer 5 neurons.\n*   **CH (Chattering)**: High-frequency bursts (gamma oscillations).\n\n## Dendritic Neurons (Active Dendrites)\n\nThe most advanced model in NeuroxAI. It treats the neuron not as a point, but as a tree.\n\n### Architecture\n*   **Soma**: Integrates inputs from branches.\n*   **Dendritic Branches**: Independent computational units.\n\n### Nonlinear Integration (NMDA Spikes)\nDendrites are not linear summers. They can generate **Plateau Potentials**.\n\n1.  **Clustering**: If 10+ synapses on a single branch fire within 50ms (Spatiotemporal Cluster).\n2.  **NMDA Spike**: Magnesium block is removed, Calcium floods the branch.\n3.  **Plateau**: The branch voltage jumps and holds for ~100ms.\n4.  **Effect**: Input from this branch is amplified by **3-5x**.\n\n**Significance**: This turns a single neuron into a 2-layer neural network, vastly increasing computational capacity (solving XOR problems within a single cell).\n","/docs/modules/problem_solving.md":"# Cognitive Symbolic Engine\n\nThe **Cognitive Symbolic Engine** acts as the \"System 2\" reasoning cortex for NeuroxAI. While the neural spiking networks handle probabilistic pattern matching and intuition, this engine handles precision, logic, and symbolic manipulation.\n\nIt bridges the gap between biological plausibility and the need for exact analytical answers in AGI systems.\n\n## Symbolic Math Engine\nThe `MathSolver` is not a calculator; it is a recursive **Abstract Syntax Tree (AST)** processor capable of algebraic manipulation.\n\n### Capabilities\n*   **Symbolic Differentiation**: Calculates derivatives of complex functions.\n    *   *Example*: `diff(x^2 + sin(x), x)` → `2*x + cos(x)`\n*   **AST Simplification**: intelligently reduces expressions.\n*   **Self-Verification (Reality Check)**: After solving an equation, the engine performs a **back-substitution** (LHS vs RHS check) to prove the result's validity.\n\n## Computational Chemistry\nThe `ChemistrySolver` integrates a static database of physical properties with linear algebra solvers to simulate chemical reasoning.\n\n### Capabilities\n*   **Stoichiometric Balancing**: Automatically calculates coefficients.\n*   **Mass Conservation Verification**: After balancing, the engine calculates the exact mass of reactants and products (in g/mol) to verify the **Law of Conservation of Mass**.\n*   **Deep Analysis**: Parses molecular formulas to compute molar mass and elemental composition.\n\n## Integration Philosophy\nIn the human brain, explicit calculation recruits specific cortical regions (like the intraparietal sulcus) distinct from intuitive processing. NeuroxAI simulates this by having the `CognitiveSystem` delegate specific queries (detected via grammar or intent) to this Symbolic Engine, injecting the precise results back into the neural stream as \"context\".","/docs/modules/reasoning_motivation.md":"# Abstract Reasoning & Motivation\n\nNeuroxAI extends beyond simple pattern recognition by incorporating modules for symbolic reasoning and intrinsic motivation. These systems allow the agent to solve novel problems and explore its environment autonomously.\n\n## Abstract Reasoning\n\nThe reasoning engine combines neural representations with symbolic logic rules (\"Neuro-Symbolic AI\").\n\n### Analogy Engine\nSolves problems of the form **A : B :: C : ?**.\n*   **Mechanism**: Vector arithmetic in semantic space.\n*   **Equation**: $\\vec{D} = \\vec{C} + (\\vec{B} - \\vec{A})$\n*   **Confidence**: Weighted by the magnitude of the transformation vector.\n\n### Logical Inference\nMaintains a knowledge base of **Facts** and **Rules**.\n*   **Facts**: Triples of `(Subject, Relation, Object)`. E.g., `(Dog, IsA, Animal)`.\n*   **Rules**: Templates for inferring new facts.\n    *   *Transitivity*: `(X, IsA, Y) & (Y, IsA, Z) -> (X, IsA, Z)`\n    *   *Inheritance*: `(X, IsA, Y) & (Y, HasA, Z) -> (X, HasA, Z)`\n    *   *Causation*: `(X, Causes, Y) & (Y, Causes, Z) -> (X, Causes, Z)`\n\n### Compositional Reasoning\nCreates new concepts by combining existing ones.\n*   **Weighted Composition**: $Concept_{new} = w_A \\cdot \\vec{A} + w_B \\cdot \\vec{B}$\n*   The weights depend on the relation type (e.g., `HasA` dominates the composition more than `IsA`).\n\n## Curiosity & Intrinsic Motivation\n\nThe agent is not just reactive; it possesses an internal drive to learn.\n\n### The \"Goldilocks\" Principle\nThe system seeks situations that are **neither too simple nor too chaotic**.\n*   **Too Simple**: Zero prediction error $\\to$ Low reward (Boring).\n*   **Too Chaotic**: Unreducible error $\\to$ Low reward (Frustrating).\n*   **Just Right**: High **Learning Progress** (Error is decreasing).\n\n### Components of Curiosity\n1.  **Novelty**: $1.0 - \\max(CosineSimilarity(Current, History))$.\n2.  **Competence Progress**: The derivative of prediction error over time.\n    $$ Reward_{intrinsic} = \\frac{-d(Error)}{dt} $$\n3.  **Exploration Bonus**: Increases the randomness of action selection when curiosity is high.\n\n### State Storage\nThe system maintains a buffer of \"Interesting States\" (high novelty/error) to revisit later (e.g., during sleep replay).\n","/docs/modules/spatial_navigation.md":"# Spatial Navigation\n\nThe spatial system provides the agent with a sense of \"where\" it is, supporting both physical navigation and abstract cognitive mapping.\n\n## Grid Cells (Entorhinal Cortex)\n\nGrid cells provide a metric coordinate system for the brain.\n\n*   **Firing Pattern**: Hexagonal lattice. A single cell fires at multiple locations forming a perfect grid.\n*   **Mechanism**: Implemented via **Path Integration**.\n    *   Input: Velocity vector $(v_x, v_y)$.\n    *   Math: Sum of 3 cosine waves oscillating at $60^{\\circ}$ intervals.\n*   **Modules**: We simulate multiple grid modules with different scales (spacings).\n    *   Small scale: ~30cm.\n    *   Large scale: ~10m.\n    *   Scale ratio: $\\approx 1.42$ (optimal for resolution).\n\n## Place Cells (Hippocampus)\n\nPlace cells represent specific locations in a specific context.\n\n*   **Firing Pattern**: Gaussian field. Fires only when the agent is in a specific spot.\n*   **Formation**: Derived from the intersection of multiple Grid Cell inputs.\n*   **Remapping**: When the environment changes (e.g., lights go out, room shape changes), Place Cells \"remap\" (assign to new random locations), creating a unique orthogonal code for that context.\n\n## Semantic Spaces (The \"Cognitive Map\")\n\nRecent research suggests the brain uses grid/place codes for non-spatial data too. NeuroxAI implements this **Semantic Grid**.\n\n*   **Concept Mapping**: Words and concepts are mapped into a 2D/3D manifold.\n*   **Navigation**: \"Thinking\" becomes navigation through concept space.\n*   **Distance**: Semantic similarity = Euclidean distance in grid space.\n","/docs/modules/spiking_cnn.md":"# Spiking CNN Module\r\n\r\nThe `SpikingCNN` module introduces a biologically-plausible Convolutional Neural Network architecture that operates entirely on spike timing. Unlike traditional CNNs that use continuous activation functions (ReLU, Sigmoid), this architecture uses Leaky Integrate-and-Fire (LIF) neurons and spike-based pooling.\r\n\r\nThis module is fully GPU-accelerated using CUDA kernels, providing 100x+ speedups over CPU implementations.\r\n\r\n## Architecture\r\n\r\nThe standard architecture follows a VGG-like pattern adapted for SNNs:\r\n\r\n1. **Input Layer**: 28x28 Spike Grid (Poisson-encoded)\r\n2. **Conv1**: 32 filters (3x3), LIF dynamics\r\n3. **Pool1**: 2x2 Max Pooling (Spike-based)\r\n4. **Conv2**: 64 filters (3x3), LIF dynamics\r\n5. **Pool2**: 2x2 Max Pooling (Spike-based)\r\n6. **Flatten**: Conversion to dense spike vector\r\n7. **Dense**: Fully connected layer (Output)\r\n\r\n## GPU Acceleration\r\n\r\nThe core operations are implemented in `src/brain/cuda/spiking_conv_kernels.rs`:\r\n\r\n### `GpuSpikingConv2D`\r\n\r\nImplements a fused convolution + LIF kernel:\r\n\r\n* **Convolution**: Standard sliding window accumulation.\r\n* **LIF Dynamics**: Membrane potential integration `dV = (-V + I)/tau`.\r\n* **Spike Generation**: Threshold check and reset.\r\n\r\nAll state variables (membrane potential, spikes) are kept in GPU memory to avoid host-device transfers.\r\n\r\n### `GpuSpikeMaxPool`\r\n\r\nImplements spike-based pooling:\r\n\r\n* Propagates the *strongest* spike within the pooling window.\r\n* Preserves temporal information (earliest/strongest spike wins).\r\n\r\n## Usage\r\n\r\nTo use the Spiking CNN in your model:\r\n\r\n```rust\r\nuse neurox_ai::brain::cuda::GpuSpikingConv2D;\r\n\r\n// Initialize layer\r\nlet mut conv1 = GpuSpikingConv2D::new(\r\n    device.clone(),\r\n    1,   // In channels\r\n    32,  // Out channels\r\n    3,   // Kernel size\r\n    28,  // Height\r\n    28   // Width\r\n)?;\r\n\r\n// Forward pass\r\nlet output_spikes = conv1.forward(&input_spikes_gpu)?;\r\n```\r\n","/docs/modules/subcortical_systems.md":"# Subcortical Systems\n\nThe cortex does not act alone. NeuroxAI simulates the ancient, subcortical loops essential for survival, motivation, and motor control.\n\n## Basal Ganglia (Action Selection)\n\nThe Basal Ganglia (BG) selects the most appropriate action based on the current state and expected reward. It implements a biological Reinforcement Learning (RL) algorithm.\n\n### Pathways\n1.  **Direct Pathway (\"Go\")**:\n    *   **Striatum D1** $\\to$ **GPi** $\\to$ **Thalamus**.\n    *   Disinhibits the thalamus, allowing an action to proceed.\n    *   Learning: Potentiated by **high Dopamine** (positive reward error).\n2.  **Indirect Pathway (\"NoGo\")**:\n    *   **Striatum D2** $\\to$ **GPe** $\\to$ **STN** $\\to$ **GPi**.\n    *   Inhibits the thalamus, suppressing actions.\n    *   Learning: Potentiated by **low Dopamine** (negative reward error).\n\n### Dopamine System\nSimulates the Ventral Tegmental Area (VTA) and Substantia Nigra (SNc).\n*   **TD Error**: $\\delta(t) = r(t) + \\gamma V(t+1) - V(t)$\n*   **Bursting**: Positive $\\delta$ causes high-frequency bursts (30Hz+).\n*   **Pausing**: Negative $\\delta$ causes pauses in firing (<1Hz).\n\n## Thalamus (The Gatekeeper)\n\nThe Thalamus is the central relay station for sensory information and cortical loops.\n\n*   **Sensory Relay**: V1, Cochlea, and Somatosensory inputs must pass through thalamic nuclei (LGN, MGN, VPM) to reach the cortex.\n*   **Attention Gating**: The Thalamic Reticular Nucleus (TRN) inhibits relay neurons, filtering out irrelevant stimuli based on Top-Down cortical signals.\n\n## Cerebellum (Motor Error Correction)\n\nThe \"little brain\" ensures smooth, coordinated movement.\n\n*   **Architecture**:\n    *   **Mossy Fibers**: Carry state/context (from Cortex/Spinal cord).\n    *   **Granule Cells**: Expand state into a high-dimensional sparse representation.\n    *   **Purkinje Cells**: Learn to predict error signals.\n    *   **Climbing Fibers**: Carry the \"teaching signal\" (retinal slip, motor error).\n*   **Function**: Computes a \"predictive correction\" term that is added to the motor command, minimizing error over time.\n\n## Amygdala (Emotional Processing)\n\nProcesses fear, threat, and salience.\n\n*   **Input**: Receives coarse, fast sensory data directly from the Thalamus (Low Road) and detailed data from Cortex (High Road).\n*   **Output**:\n    *   **Central Nucleus**: Triggers autonomic responses (via Hypothalamus/Brainstem) -> Increases Norepinephrine.\n    *   **Basolateral Nucleus**: Modulates memory consolidation in the Hippocampus (emotional tagging).\n","/docs/overview.md":"# Introduction to NeuroxAI\n\n**NeuroxAI** is a cutting-edge, biologically plausible neuromorphic computing platform designed to bridge the gap between biological brains and artificial intelligence. Unlike traditional deep learning models that rely on backpropagation and continuous activation functions, NeuroxAI simulates the precise temporal dynamics of Spiking Neural Networks (SNNs) integrated with complex neuroanatomical structures.\n\nBuilt in **Rust** for safety and performance, and accelerated by **CUDA**, NeuroxAI targets real-time simulation of 1-10 million neurons with detailed synaptic physiology.\n\n## Core Philosophy\n\nNeuroxAI is built on the hypothesis that **General Intelligence** emerges not from a single algorithm, but from the interaction of specialized, evolutionarily conserved systems.\n\n1.  **Biological Plausibility**: We strictly adhere to neuroscience constraints. Learning happens locally (STDP), information is sparse (spikes), and homeostasis maintains stability.\n2.  **Embodied Cognition**: The brain does not exist in a vacuum. NeuroxAI simulates sensory streams (vision, audio, somatic) and motor outputs.\n3.  **Active Inference**: The brain is a prediction machine. Our architecture is fundamentally predictive, minimizing free energy (surprise) across all cortical levels.\n\n## Key Features\n\n### Whole-Brain Architecture\nWe move beyond simple \"layers\" to simulate distinct brain organs:\n*   **Cortex**: 6-layer microcircuitry with predictive coding.\n*   **Hippocampus**: Fast episodic memory encoding and replay.\n*   **Basal Ganglia**: Action selection and reinforcement learning.\n*   **Cerebellum**: Fine-motor control and error correction.\n*   **Thalamus**: The \"switchboard\" of consciousness and attention.\n\n### Cognitive Symbolic Engine\n*   **Symbolic Math**: AST-based engine for calculus (`diff`), algebra, and simplification.\n*   **Scientific Reasoning**: Stoichiometry, molar mass calculation, and step-by-step chemical analysis.\n\n### Benchmarks\n*   **MNIST**: Integrated benchmarking suite with quantization (4/8-bit) and synthetic data generation.\n\n### Synaptic & Structural Plasticity\n*   **STDP (Spike-Timing-Dependent Plasticity)**: Hebbian learning based on millisecond-precise timing.\n*   **Neuromodulation**: Dopamine (reward), Serotonin (patience), Norepinephrine (arousal), and Acetylcholine (attention) dynamically modulate learning rates and thresholds.\n*   **Structural Plasticity**: The network physically rewires itself—growing new synapses and pruning unused ones based on activity.\n\n### High-Performance Engineering\n*   **Rust Core**: Zero-cost abstractions and memory safety without garbage collection pauses.\n*   **CUDA Acceleration**: Custom GPU kernels for V1 visual processing and massive matrix operations, achieving **100x speedups** over CPU execution.\n*   **Sparse Computing**: Optimized data structures for handling the <1% sparsity of biological neural activity.\n\n### The Semantic Genome\nNeuroxAI is not trained on raw text noise. It uses a structured **Semantic Genome** (`data/czech_training.json`) that maps words directly to:\n*   **Neuro-Impacts**: How words affect Dopamine, Oxytocin, and Stress levels.\n*   **Pragmatics**: Rules for social interaction and logical flow.\n*   **Emotional Triggers**: Patterns that elicit Joy, Anger, or Anticipation.\n\n## Use Cases\n\n*   **Neuromorphic Research**: Testing hypotheses about brain function in silicon.\n*   **AGI Prototyping**: Developing cognitive architectures that require reasoning, memory, and adaptation.\n*   **Robotics**: Controlling agents with biologically inspired motor learning and reflex loops.\n*   **Cognitive Science**: Simulating psychological phenomena like fear conditioning, memory consolidation, and sleep.\n"}